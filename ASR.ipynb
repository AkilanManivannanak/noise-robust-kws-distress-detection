{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--VSgQm1Lxbf",
    "outputId": "a2faecbe-ce4e-452d-8a2d-03b660b0320c"
   },
   "outputs": [],
   "source": [
    "pip install jupyterlab ipykernel numpy pandas matplotlib tensorboard soundfile torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0SGzruB737T",
    "outputId": "c3849f16-69a7-48e9-e893-687c0489f0fc"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0. SETUP: IMPORTS, CONFIG, DEVICE, DIRECTORIES\n",
    "# ==============================\n",
    "\n",
    "# If you're in COLAB, uncomment and run this once:\n",
    "# !pip install soundfile torchaudio --quiet\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Device selection -----\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----- Global audio / feature config -----\n",
    "SAMPLE_RATE = 16_000          # 16 kHz standard for speech\n",
    "CLIP_DURATION_SEC = 1.0       # 1-second clips\n",
    "CLIP_NUM_SAMPLES = int(SAMPLE_RATE * CLIP_DURATION_SEC)  # 16000 samples\n",
    "\n",
    "N_MELS = 64       # number of mel filterbanks / MFCC coefficients\n",
    "N_FFT = 512       # FFT size\n",
    "HOP_LENGTH = 160  # ~10ms hop\n",
    "WIN_LENGTH = 400  # ~25ms window\n",
    "\n",
    "# Commands for 10-class KWS + \"unknown\"\n",
    "TARGET_COMMANDS = [\n",
    "    \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"\n",
    "]\n",
    "ALL_CLASSES = TARGET_COMMANDS + [\"unknown\"]     # total 11 classes\n",
    "NUM_CLASSES = len(ALL_CLASSES)\n",
    "CLASS_TO_INDEX = {c: i for i, c in enumerate(ALL_CLASSES)}\n",
    "\n",
    "# ----- Directory layout -----\n",
    "ROOT_DATA = \"./data\"\n",
    "SPEECH_COMMANDS_ROOT = os.path.join(ROOT_DATA, \"speech_commands\")\n",
    "NOISE_DIR = os.path.join(ROOT_DATA, \"noise\")\n",
    "RAVDESS_ROOT = \"./data/ravdess/audio_speech_actors_01-24\"  # adjust if your folder is different\n",
    "\n",
    "os.makedirs(ROOT_DATA, exist_ok=True)\n",
    "os.makedirs(SPEECH_COMMANDS_ROOT, exist_ok=True)\n",
    "os.makedirs(NOISE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Data root:\", ROOT_DATA)\n",
    "print(\"SpeechCommands root:\", SPEECH_COMMANDS_ROOT)\n",
    "print(\"Noise dir:\", NOISE_DIR)\n",
    "print(\"RAVDESS dir:\", RAVDESS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqx7I4Hg8sUU"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. BASIC AUDIO UTILITIES + SAFE torchaudio.load\n",
    "# ==============================\n",
    "\n",
    "def pad_or_trim(waveform: torch.Tensor, target_num_samples: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ensure waveform has exactly target_num_samples samples.\n",
    "    If longer -> truncate. If shorter -> zero-pad at the end.\n",
    "\n",
    "    waveform: [1, T] or [C, T]\n",
    "    returns:  [1, target_num_samples] (mono)\n",
    "    \"\"\"\n",
    "    # Force mono [1, T]\n",
    "    if waveform.ndim == 2 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    elif waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    num_samples = waveform.shape[-1]\n",
    "    if num_samples == target_num_samples:\n",
    "        return waveform\n",
    "    elif num_samples > target_num_samples:\n",
    "        return waveform[..., :target_num_samples]\n",
    "    else:\n",
    "        pad_amount = target_num_samples - num_samples\n",
    "        return torch.nn.functional.pad(waveform, (0, pad_amount))\n",
    "\n",
    "\n",
    "def compute_rms(x: torch.Tensor, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Root Mean Square energy of waveform.\n",
    "    x: [1, T]\n",
    "    returns: scalar rms float\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean(x ** 2) + eps).item()\n",
    "\n",
    "\n",
    "def mix_with_noise(clean: torch.Tensor, noise: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mix clean speech with noise at a desired SNR (in dB).\n",
    "\n",
    "    clean: [1, T]\n",
    "    noise: [C, T_noise] or [1, T_noise]\n",
    "    returns: [1, T] noisy waveform\n",
    "    \"\"\"\n",
    "    # Ensure mono\n",
    "    if noise.ndim == 2 and noise.shape[0] > 1:\n",
    "        noise = noise.mean(dim=0, keepdim=True)\n",
    "    elif noise.ndim == 1:\n",
    "        noise = noise.unsqueeze(0)\n",
    "\n",
    "    T = clean.shape[-1]\n",
    "\n",
    "    # Ensure noise is long enough. If too short, repeat it.\n",
    "    if noise.shape[-1] < T:\n",
    "        repeats = (T // noise.shape[-1]) + 1\n",
    "        noise = noise.repeat(1, repeats)\n",
    "\n",
    "    # Randomly crop a T-length segment of the noise\n",
    "    start = random.randint(0, noise.shape[-1] - T)\n",
    "    noise_segment = noise[..., start:start + T]\n",
    "\n",
    "    # Compute RMS levels of clean and noise\n",
    "    rms_clean = compute_rms(clean)\n",
    "    rms_noise = compute_rms(noise_segment)\n",
    "    if rms_noise == 0:\n",
    "        # Degenerate noise (silent) -> return clean\n",
    "        return clean\n",
    "\n",
    "    # SNR in dB: SNR = 20 * log10(rms_clean / rms_noise_scaled)\n",
    "    snr_linear = 10 ** (snr_db / 20.0)\n",
    "    k = rms_clean / (rms_noise * snr_linear)\n",
    "    noise_scaled = k * noise_segment\n",
    "\n",
    "    mixed = clean + noise_scaled\n",
    "    mixed = torch.clamp(mixed, -1.0, 1.0)\n",
    "    return mixed\n",
    "\n",
    "\n",
    "# ----- Monkey-patch torchaudio.load to use soundfile backend -----\n",
    "def safe_torchaudio_load(path,\n",
    "                         frame_offset=0,\n",
    "                         num_frames=-1,\n",
    "                         normalize=True,\n",
    "                         channels_first=True,\n",
    "                         format=None,\n",
    "                         buffer_size=4096,\n",
    "                         backend=None):\n",
    "    \"\"\"\n",
    "    Replacement for torchaudio.load that uses soundfile, to avoid backend issues.\n",
    "    \"\"\"\n",
    "    data, sr = sf.read(path, dtype=\"float32\", always_2d=True)  # [T, C]\n",
    "    wav = torch.from_numpy(data)  # [T, C]\n",
    "\n",
    "    # Handle frame offset / num_frames slicing\n",
    "    if frame_offset > 0 or num_frames > -1:\n",
    "        end = None if num_frames < 0 else frame_offset + num_frames\n",
    "        wav = wav[frame_offset:end]\n",
    "\n",
    "    # Convert to [C, T] if channels_first\n",
    "    if channels_first:\n",
    "        wav = wav.transpose(0, 1)  # [C, T]\n",
    "\n",
    "    # Normalization: soundfile already returns float32 in [-1, 1] for PCM\n",
    "    return wav, sr\n",
    "\n",
    "\n",
    "torchaudio.load = safe_torchaudio_load\n",
    "print(\"Patched torchaudio.load to use soundfile backend.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. FEATURE EXTRACTION: LOG-MEL + MFCC\n",
    "# ==============================\n",
    "\n",
    "# MelSpectrogram transform (for log-Mel features)\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    win_length=WIN_LENGTH,\n",
    "    n_mels=N_MELS,\n",
    ")\n",
    "\n",
    "# Convert amplitude to dB (log-like scale)\n",
    "amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "# MFCC transform (for MFCC features)\n",
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MELS,   # keep same \"freq\" dimension size so CNN architecture is unchanged\n",
    "    melkwargs={\n",
    "        \"n_fft\": N_FFT,\n",
    "        \"n_mels\": N_MELS,\n",
    "        \"hop_length\": HOP_LENGTH,\n",
    "        \"win_length\": WIN_LENGTH,\n",
    "        \"mel_scale\": \"htk\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def waveform_to_features(waveform: torch.Tensor, mode: str = \"logmel\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert waveform to 2D time-frequency features.\n",
    "\n",
    "    waveform: [1, T]\n",
    "    mode: \"logmel\" or \"mfcc\"\n",
    "    returns: [1, N_MELS, T_frames]\n",
    "    \"\"\"\n",
    "    waveform = waveform.to(torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if mode == \"logmel\":\n",
    "            mel = mel_transform(waveform)      # [1, n_mels, time]\n",
    "            feat = amp_to_db(mel)             # [1, n_mels, time]\n",
    "        elif mode == \"mfcc\":\n",
    "            mfcc = mfcc_transform(waveform)   # [1, n_mfcc, time]\n",
    "            feat = mfcc\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature mode: {mode}\")\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhjD0ucc8zYV"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. SPECAUGMENT (TIME + FREQUENCY MASKING)\n",
    "# ==============================\n",
    "\n",
    "def spec_augment(\n",
    "    feat: torch.Tensor,\n",
    "    max_time_mask: int = 20,\n",
    "    max_freq_mask: int = 8,\n",
    "    num_time_masks: int = 2,\n",
    "    num_freq_masks: int = 2,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simple SpecAugment implementation.\n",
    "\n",
    "    feat: [1, N_MELS, T]\n",
    "    returns: augmented feature of same shape\n",
    "    \"\"\"\n",
    "    out = feat.clone()\n",
    "    _, n_mels, T = out.shape\n",
    "\n",
    "    # Time masks\n",
    "    for _ in range(num_time_masks):\n",
    "        t = random.randint(0, max_time_mask)\n",
    "        if t == 0 or T - t <= 0:\n",
    "            continue\n",
    "        t0 = random.randint(0, T - t)\n",
    "        out[:, :, t0:t0 + t] = 0.0\n",
    "\n",
    "    # Frequency masks\n",
    "    for _ in range(num_freq_masks):\n",
    "        f = random.randint(0, max_freq_mask)\n",
    "        if f == 0 or n_mels - f <= 0:\n",
    "            continue\n",
    "        f0 = random.randint(0, n_mels - f)\n",
    "        out[:, f0:f0 + f, :] = 0.0\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIwlDYyo9B4g"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. LOAD NOISE FILES\n",
    "# ==============================\n",
    "\n",
    "def load_noise_waveforms(noise_dir: str, sample_rate: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Load all .wav files in noise_dir, convert to mono and resample.\n",
    "\n",
    "    returns: list of waveforms, each [1, T_noise]\n",
    "    \"\"\"\n",
    "    noise_waveforms = []\n",
    "    noise_dir_path = Path(noise_dir)\n",
    "    if not noise_dir_path.exists():\n",
    "        print(f\"[WARN] Noise directory {noise_dir} does not exist.\")\n",
    "        return noise_waveforms\n",
    "\n",
    "    for wav_path in noise_dir_path.glob(\"*.wav\"):\n",
    "        try:\n",
    "            wav, sr = torchaudio.load(str(wav_path))\n",
    "\n",
    "            # Convert to mono if multi-channel\n",
    "            wav = pad_or_trim(wav, wav.shape[-1])  # ensure [1, T]\n",
    "\n",
    "            # Resample if needed\n",
    "            if sr != sample_rate:\n",
    "                wav = torchaudio.functional.resample(wav, sr, sample_rate)\n",
    "\n",
    "            # Remove DC offset (center around 0)\n",
    "            wav = wav - wav.mean()\n",
    "            noise_waveforms.append(wav)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load noise file {wav_path}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(noise_waveforms)} noise files from {noise_dir}\")\n",
    "    return noise_waveforms\n",
    "\n",
    "\n",
    "NOISE_WAVEFORMS = load_noise_waveforms(NOISE_DIR, SAMPLE_RATE)\n",
    "if len(NOISE_WAVEFORMS) == 0:\n",
    "    print(\"NOISE_WAVEFORMS is empty – KWS noise training will still run but effectively be clean.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvZntCBT9DsB",
    "outputId": "54c881aa-4b7c-49a7-a902-3050d066839b"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5. SPEECH COMMANDS DATASET + COLLATE + LOADERS\n",
    "# ==============================\n",
    "\n",
    "class GuardianSpeechCommands(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around torchaudio.datasets.SPEECHCOMMANDS with:\n",
    "      - 11-class label mapping (10 commands + 'unknown')\n",
    "      - optional noise mixing (for KWS multi-condition training)\n",
    "      - log-Mel or MFCC feature extraction\n",
    "      - optional SpecAugment (for log-Mel)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        subset: str,                 # \"training\", \"validation\", \"testing\"\n",
    "        target_commands,\n",
    "        all_classes,\n",
    "        noise_waveforms=None,        # list of noise waveforms or None\n",
    "        p_noise: float = 0.7,        # probability of mixing noise during training\n",
    "        snr_db_range=(0, 20),        # SNR range for noise mixing\n",
    "        sample_rate: int = SAMPLE_RATE,\n",
    "        feature_mode: str = \"logmel\",    # \"logmel\" or \"mfcc\"\n",
    "        use_specaugment: bool = False,   # apply SpecAugment (for logmel only)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert subset in [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "        self.base = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "            root=root,\n",
    "            download=True,\n",
    "            url=\"speech_commands_v0.02\",\n",
    "            subset=subset,\n",
    "        )\n",
    "        self.target_commands = set(target_commands)\n",
    "        self.all_classes = all_classes\n",
    "        self.class_to_index = {c: i for i, c in enumerate(all_classes)}\n",
    "\n",
    "        self.noise_waveforms = noise_waveforms if noise_waveforms is not None else []\n",
    "        self.p_noise = p_noise\n",
    "        self.snr_db_range = snr_db_range\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.feature_mode = feature_mode\n",
    "        self.use_specaugment = use_specaugment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def _map_label(self, label: str) -> int:\n",
    "        \"\"\"\n",
    "        Map raw label string to:\n",
    "        - its own index if in TARGET_COMMANDS\n",
    "        - \"unknown\" class index otherwise\n",
    "        \"\"\"\n",
    "        if label in self.target_commands:\n",
    "            return self.class_to_index[label]\n",
    "        else:\n",
    "            return self.class_to_index[\"unknown\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # base[idx] returns: (waveform, sample_rate, label, speaker_id, utterance_number)\n",
    "        waveform, sr, label, speaker_id, utt_number = self.base[idx]\n",
    "\n",
    "        waveform = pad_or_trim(waveform, waveform.shape[-1])  # ensure [1, T]\n",
    "\n",
    "        # Resample to desired SAMPLE_RATE if needed\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        # Pad/trim to fixed length (1 second)\n",
    "        waveform = pad_or_trim(waveform, CLIP_NUM_SAMPLES)\n",
    "\n",
    "        # Optional noise mixing (mainly for training)\n",
    "        if self.noise_waveforms and random.random() < self.p_noise:\n",
    "            snr_db = random.uniform(*self.snr_db_range)\n",
    "            noise = random.choice(self.noise_waveforms)\n",
    "            waveform = mix_with_noise(waveform, noise, snr_db)\n",
    "\n",
    "        # Convert to time-frequency features (log-Mel or MFCC)\n",
    "        feat = waveform_to_features(waveform, mode=self.feature_mode)  # [1, F, T]\n",
    "\n",
    "        # Optional SpecAugment (for training log-Mel)\n",
    "        if self.use_specaugment and self.feature_mode == \"logmel\":\n",
    "            if random.random() < 0.5:\n",
    "                feat = spec_augment(feat)\n",
    "\n",
    "        # Map label string to integer class index\n",
    "        y = self._map_label(label)\n",
    "\n",
    "        return feat, y\n",
    "\n",
    "\n",
    "def guardian_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable time dimension.\n",
    "\n",
    "    batch: list of tuples (feat [1,F,T_i], label)\n",
    "    returns:\n",
    "      X: [B, 1, F, T_max]\n",
    "      y: [B]\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    max_T = max(x.shape[-1] for x in xs)\n",
    "\n",
    "    padded = []\n",
    "    for x in xs:\n",
    "        T = x.shape[-1]\n",
    "        if T < max_T:\n",
    "            pad_amount = max_T - T\n",
    "            x = torch.nn.functional.pad(x, (0, pad_amount))\n",
    "        padded.append(x)\n",
    "\n",
    "    X = torch.stack(padded, dim=0)          # [B, 1, F, T_max]\n",
    "    y = torch.tensor(ys, dtype=torch.long)  # [B]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_loaders(\n",
    "    feature_mode=\"logmel\",\n",
    "    train_with_noise=False,\n",
    "    use_specaugment=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build train/val/test DataLoaders for SpeechCommands.\n",
    "\n",
    "    - train_with_noise: if True, training set uses noise mixing.\n",
    "    - use_specaugment: if True, SpecAugment applied on training features (log-Mel only).\n",
    "    \"\"\"\n",
    "    if train_with_noise and len(NOISE_WAVEFORMS) > 0:\n",
    "        train_noise_waveforms = NOISE_WAVEFORMS\n",
    "        p_noise = 0.7\n",
    "        snr_range = (0, 20)\n",
    "    else:\n",
    "        train_noise_waveforms = None\n",
    "        p_noise = 0.0\n",
    "        snr_range = (0, 0)\n",
    "\n",
    "    train_dataset = GuardianSpeechCommands(\n",
    "        root=SPEECH_COMMANDS_ROOT,\n",
    "        subset=\"training\",\n",
    "        target_commands=TARGET_COMMANDS,\n",
    "        all_classes=ALL_CLASSES,\n",
    "        noise_waveforms=train_noise_waveforms,\n",
    "        p_noise=p_noise,\n",
    "        snr_db_range=snr_range,\n",
    "        feature_mode=feature_mode,\n",
    "        use_specaugment=use_specaugment,\n",
    "    )\n",
    "\n",
    "    val_dataset = GuardianSpeechCommands(\n",
    "        root=SPEECH_COMMANDS_ROOT,\n",
    "        subset=\"validation\",\n",
    "        target_commands=TARGET_COMMANDS,\n",
    "        all_classes=ALL_CLASSES,\n",
    "        noise_waveforms=None,\n",
    "        p_noise=0.0,\n",
    "        snr_db_range=(0, 0),\n",
    "        feature_mode=feature_mode,\n",
    "        use_specaugment=False,\n",
    "    )\n",
    "\n",
    "    test_dataset = GuardianSpeechCommands(\n",
    "        root=SPEECH_COMMANDS_ROOT,\n",
    "        subset=\"testing\",\n",
    "        target_commands=TARGET_COMMANDS,\n",
    "        all_classes=ALL_CLASSES,\n",
    "        noise_waveforms=None,\n",
    "        p_noise=0.0,\n",
    "        snr_db_range=(0, 0),\n",
    "        feature_mode=feature_mode,\n",
    "        use_specaugment=False,\n",
    "    )\n",
    "\n",
    "    pin_mem = torch.cuda.is_available()  # only relevant for CUDA\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "        collate_fn=guardian_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "        collate_fn=guardian_collate_fn,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "        collate_fn=guardian_collate_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"[make_loaders] feature_mode={feature_mode}, \"\n",
    "          f\"train_with_noise={train_with_noise}, use_specaugment={use_specaugment}\")\n",
    "    print(\"  Train examples:\", len(train_dataset))\n",
    "    print(\"  Val   examples:\", len(val_dataset))\n",
    "    print(\"  Test  examples:\", len(test_dataset))\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5fB5e2rLwCD"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7. MODEL: SHARED ENCODER + COMMAND HEAD\n",
    "# ==============================\n",
    "\n",
    "class SharedAudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int = N_MELS):\n",
    "        super().__init__()\n",
    "        # Conv block 1: 1 -> 32 channels\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "        )\n",
    "        # Conv block 2: 32 -> 64 channels\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "        )\n",
    "        # Conv block 3: 64 -> 128 channels\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = 128  # after global average pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, F, T]\n",
    "        x = self.conv_block1(x)          # [B,32, F/2, T/2]\n",
    "        x = self.conv_block2(x)          # [B,64, F/4, T/4]\n",
    "        x = self.conv_block3(x)          # [B,128,F/8, T/8]\n",
    "        # Global average pooling over freq and time -> [B,128]\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        return x\n",
    "\n",
    "\n",
    "class CommandHead(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class GuardianVoiceModel(nn.Module):\n",
    "    def __init__(self, num_command_classes: int):\n",
    "        super().__init__()\n",
    "        self.encoder = SharedAudioEncoder(n_mels=N_MELS)\n",
    "        self.command_head = CommandHead(self.encoder.embedding_dim, num_command_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,1,F,T]\n",
    "        z = self.encoder(x)             # [B,128]\n",
    "        cmd_logits = self.command_head(z)   # [B,num_classes]\n",
    "        return cmd_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 8. TRAINING & EVALUATION HELPERS\n",
    "# ==============================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    One training epoch over loader.\n",
    "    Returns (avg_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on loader.\n",
    "    Returns (avg_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 9. BUILD NOISY TEST LOADERS (FOR SNR EVAL)\n",
    "# ==============================\n",
    "\n",
    "def build_noisy_test_loader(\n",
    "    feature_mode: str,\n",
    "    snr_db: float,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a test loader where all examples are mixed with noise\n",
    "    at a fixed SNR.\n",
    "    \"\"\"\n",
    "    noisy_test_ds = GuardianSpeechCommands(\n",
    "        root=SPEECH_COMMANDS_ROOT,\n",
    "        subset=\"testing\",\n",
    "        target_commands=TARGET_COMMANDS,\n",
    "        all_classes=ALL_CLASSES,\n",
    "        noise_waveforms=NOISE_WAVEFORMS,\n",
    "        p_noise=1.0,                      # always mix noise\n",
    "        snr_db_range=(snr_db, snr_db),    # fixed SNR\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        feature_mode=feature_mode,\n",
    "        use_specaugment=False,\n",
    "    )\n",
    "    noisy_test_loader = DataLoader(\n",
    "        noisy_test_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=guardian_collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    print(f\"Built noisy test loader for feature_mode={feature_mode}, \"\n",
    "          f\"SNR={snr_db} dB, size={len(noisy_test_ds)}\")\n",
    "    return noisy_test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10.1 EXP 1: LOG-MEL, CLEAN-ONLY TRAINING\n",
    "# ==============================\n",
    "\n",
    "criterion_kws = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader_clean, val_loader_logmel_clean, test_loader_logmel_clean = make_loaders(\n",
    "    feature_mode=\"logmel\",\n",
    "    train_with_noise=False,\n",
    "    use_specaugment=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model_clean = GuardianVoiceModel(num_command_classes=NUM_CLASSES).to(device)\n",
    "optimizer_clean = torch.optim.Adam(model_clean.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_clean = 5\n",
    "\n",
    "for epoch in range(1, num_epochs_clean + 1):\n",
    "    train_loss_c, train_acc_c = train_one_epoch(\n",
    "        model_clean, train_loader_clean, optimizer_clean, criterion_kws, device\n",
    "    )\n",
    "    val_loss_c, val_acc_c = evaluate(model_clean, val_loader_logmel_clean, criterion_kws, device)\n",
    "    print(\n",
    "        f\"[CLEAN-LOGMEL] Epoch {epoch:02d}: \"\n",
    "        f\"Train loss {train_loss_c:.4f}, acc {train_acc_c*100:.2f}% | \"\n",
    "        f\"Val loss {val_loss_c:.4f}, acc {val_acc_c*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "clean_test_loss_c, clean_test_acc_c = evaluate(\n",
    "    model_clean, test_loader_logmel_clean, criterion_kws, device\n",
    ")\n",
    "print(\n",
    "    f\"[CLEAN-LOGMEL] Final CLEAN test: loss {clean_test_loss_c:.4f}, \"\n",
    "    f\"acc {clean_test_acc_c*100:.2f}%\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10.2 EXP 2: LOG-MEL, NOISE-TRAINED (MULTI-CONDITION)\n",
    "# ==============================\n",
    "\n",
    "train_loader_noise, val_loader_logmel_noise, test_loader_logmel_noise = make_loaders(\n",
    "    feature_mode=\"logmel\",\n",
    "    train_with_noise=True,\n",
    "    use_specaugment=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model_noise = GuardianVoiceModel(num_command_classes=NUM_CLASSES).to(device)\n",
    "optimizer_noise = torch.optim.Adam(model_noise.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_noise = 5\n",
    "\n",
    "for epoch in range(1, num_epochs_noise + 1):\n",
    "    train_loss_n, train_acc_n = train_one_epoch(\n",
    "        model_noise, train_loader_noise, optimizer_noise, criterion_kws, device\n",
    "    )\n",
    "    val_loss_n, val_acc_n = evaluate(\n",
    "        model_noise, val_loader_logmel_noise, criterion_kws, device\n",
    "    )\n",
    "    print(f\"[NOISE-LOGMEL] Epoch {epoch:02d}: \"\n",
    "          f\"Train loss {train_loss_n:.4f}, acc {train_acc_n*100:.2f}% | \"\n",
    "          f\"Val loss {val_loss_n:.4f}, acc {val_acc_n*100:.2f}%\")\n",
    "\n",
    "clean_test_loss_n, clean_test_acc_n = evaluate(\n",
    "    model_noise, test_loader_logmel_noise, criterion_kws, device\n",
    ")\n",
    "print(f\"[NOISE-LOGMEL] Final CLEAN test: loss {clean_test_loss_n:.4f}, \"\n",
    "      f\"acc {clean_test_acc_n*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10.3 EXP 3: LOG-MEL, NOISE + SPECAUGMENT\n",
    "# ==============================\n",
    "\n",
    "train_loader_spec, val_loader_logmel_spec, test_loader_logmel_spec = make_loaders(\n",
    "    feature_mode=\"logmel\",\n",
    "    train_with_noise=True,\n",
    "    use_specaugment=True,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model_spec = GuardianVoiceModel(num_command_classes=NUM_CLASSES).to(device)\n",
    "optimizer_spec = torch.optim.Adam(model_spec.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_spec = 5\n",
    "\n",
    "for epoch in range(1, num_epochs_spec + 1):\n",
    "    train_loss_s, train_acc_s = train_one_epoch(\n",
    "        model_spec, train_loader_spec, optimizer_spec, criterion_kws, device\n",
    "    )\n",
    "    val_loss_s, val_acc_s = evaluate(\n",
    "        model_spec, val_loader_logmel_spec, criterion_kws, device\n",
    "    )\n",
    "    print(f\"[SPECAUG-LOGMEL] Epoch {epoch:02d}: \"\n",
    "          f\"Train loss {train_loss_s:.4f}, acc {train_acc_s*100:.2f}% | \"\n",
    "          f\"Val loss {val_loss_s:.4f}, acc {val_acc_s*100:.2f}%\")\n",
    "\n",
    "clean_test_loss_s, clean_test_acc_s = evaluate(\n",
    "    model_spec, test_loader_logmel_spec, criterion_kws, device\n",
    ")\n",
    "print(f\"[SPECAUG-LOGMEL] Final CLEAN test: loss {clean_test_loss_s:.4f}, \"\n",
    "      f\"acc {clean_test_acc_s*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10.4 BUILD NOISY TEST LOADERS AND EVAL (LOG-MEL)\n",
    "# ==============================\n",
    "\n",
    "snrs = [20.0, 10.0, 0.0]\n",
    "noisy_loaders_logmel = {snr: build_noisy_test_loader(\"logmel\", snr) for snr in snrs}\n",
    "\n",
    "# Evaluate all three log-Mel models (clean, noise-trained, specaug) on noisy test sets\n",
    "\n",
    "clean_accuracies = {\"Clean\": clean_test_acc_c}\n",
    "noise_accuracies = {\"Clean\": clean_test_acc_n}\n",
    "specaug_accuracies = {\"Clean\": clean_test_acc_s}\n",
    "\n",
    "for snr in snrs:\n",
    "    _, acc_c = evaluate(model_clean, noisy_loaders_logmel[snr], criterion_kws, device)\n",
    "    _, acc_n = evaluate(model_noise, noisy_loaders_logmel[snr], criterion_kws, device)\n",
    "    _, acc_s = evaluate(model_spec, noisy_loaders_logmel[snr], criterion_kws, device)\n",
    "    clean_accuracies[snr] = acc_c\n",
    "    noise_accuracies[snr] = acc_n\n",
    "    specaug_accuracies[snr] = acc_s\n",
    "\n",
    "print(\"\\n[CLEAN-LOGMEL] Acc vs SNR:\", clean_accuracies)\n",
    "print(\"[NOISE-LOGMEL] Acc vs SNR:\", noise_accuracies)\n",
    "print(\"[SPECAUG-LOGMEL] Acc vs SNR:\", specaug_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10.5  MFCC, NOISE-TRAINED (FEATURE COMPARISON)\n",
    "# ==============================\n",
    "\n",
    "train_loader_mfcc, val_loader_mfcc, test_loader_mfcc = make_loaders(\n",
    "    feature_mode=\"mfcc\",\n",
    "    train_with_noise=True,\n",
    "    use_specaugment=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model_mfcc = GuardianVoiceModel(num_command_classes=NUM_CLASSES).to(device)\n",
    "optimizer_mfcc = torch.optim.Adam(model_mfcc.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_mfcc = 5\n",
    "\n",
    "for epoch in range(1, num_epochs_mfcc + 1):\n",
    "    train_loss_m, train_acc_m = train_one_epoch(\n",
    "        model_mfcc, train_loader_mfcc, optimizer_mfcc, criterion_kws, device\n",
    "    )\n",
    "    val_loss_m, val_acc_m = evaluate(\n",
    "        model_mfcc, val_loader_mfcc, criterion_kws, device\n",
    "    )\n",
    "    print(f\"[MFCC-NOISE] Epoch {epoch:02d}: \"\n",
    "          f\"Train loss {train_loss_m:.4f}, acc {train_acc_m*100:.2f}% | \"\n",
    "          f\"Val loss {val_loss_m:.4f}, acc {val_acc_m*100:.2f}%\")\n",
    "\n",
    "clean_test_loss_m, clean_test_acc_m = evaluate(\n",
    "    model_mfcc, test_loader_mfcc, criterion_kws, device\n",
    ")\n",
    "print(f\"[MFCC-NOISE] Final CLEAN test: loss {clean_test_loss_m:.4f}, \"\n",
    "      f\"acc {clean_test_acc_m*100:.2f}%\")\n",
    "\n",
    "# Noisy MFCC loaders for SNR eval\n",
    "noisy_loaders_mfcc = {snr: build_noisy_test_loader(\"mfcc\", snr) for snr in snrs}\n",
    "\n",
    "mfcc_accuracies = {\"Clean\": clean_test_acc_m}\n",
    "for snr in snrs:\n",
    "    _, acc_m = evaluate(model_mfcc, noisy_loaders_mfcc[snr], criterion_kws, device)\n",
    "    mfcc_accuracies[snr] = acc_m\n",
    "\n",
    "print(\"[MFCC-NOISE] Acc vs SNR:\", mfcc_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 11. CONFUSION MATRIX / CLASSIFICATION REPORT (BEST MODEL)\n",
    "# ==============================\n",
    "\n",
    "# Use best KWS model: log-Mel + noise + SpecAugment (model_spec)\n",
    "model_spec.eval()\n",
    "all_y = []\n",
    "all_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader_logmel_spec:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model_spec(X)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_pred.append(preds.cpu().numpy())\n",
    "\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "\n",
    "print(\"Confusion matrix (SpecAug log-Mel model on clean test):\")\n",
    "print(confusion_matrix(all_y, all_pred))\n",
    "\n",
    "print(\"\\nClassification report (SpecAug log-Mel model on clean test):\")\n",
    "print(classification_report(all_y, all_pred, target_names=ALL_CLASSES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 12. PLOT: ACCURACY VS SNR (LOG-MEL MODELS)\n",
    "# ==============================\n",
    "\n",
    "snr_labels = [\"Clean\", \"20 dB\", \"10 dB\", \"0 dB\"]\n",
    "\n",
    "acc_clean_only = [\n",
    "    clean_accuracies[\"Clean\"] * 100,\n",
    "    clean_accuracies[20.0] * 100,\n",
    "    clean_accuracies[10.0] * 100,\n",
    "    clean_accuracies[0.0] * 100,\n",
    "]\n",
    "acc_noise_trained = [\n",
    "    noise_accuracies[\"Clean\"] * 100,\n",
    "    noise_accuracies[20.0] * 100,\n",
    "    noise_accuracies[10.0] * 100,\n",
    "    noise_accuracies[0.0] * 100,\n",
    "]\n",
    "acc_specaug = [\n",
    "    specaug_accuracies[\"Clean\"] * 100,\n",
    "    specaug_accuracies[20.0] * 100,\n",
    "    specaug_accuracies[10.0] * 100,\n",
    "    specaug_accuracies[0.0] * 100,\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(snr_labels, acc_clean_only, marker=\"o\", label=\"Clean-only (log-Mel)\")\n",
    "plt.plot(snr_labels, acc_noise_trained, marker=\"o\", label=\"Noise-trained (log-Mel)\")\n",
    "plt.plot(snr_labels, acc_specaug, marker=\"o\", label=\"Noise+SpecAug (log-Mel)\")\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"GuardianDrive-Voice: Accuracy vs SNR\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 13. GUARDIANDRIVE DEMO ON YOUR OWN WAVs\n",
    "# ==============================\n",
    "\n",
    "def load_wav_and_features(path, mode=\"logmel\", sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Load a wav file, convert to mono, resample, pad/trim,\n",
    "    and compute features (log-Mel or MFCC).\n",
    "    \"\"\"\n",
    "    wav, sr = torchaudio.load(path)\n",
    "\n",
    "    wav = pad_or_trim(wav, wav.shape[-1])  # [1, T]\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != sample_rate:\n",
    "        wav = torchaudio.functional.resample(wav, sr, sample_rate)\n",
    "\n",
    "    # Pad/trim to 1 second\n",
    "    wav = pad_or_trim(wav, CLIP_NUM_SAMPLES)\n",
    "\n",
    "    # Convert to features\n",
    "    feat = waveform_to_features(wav, mode=mode)  # [1,F,T]\n",
    "    return wav, feat\n",
    "\n",
    "\n",
    "def guardian_predict(model, feat_tensor):\n",
    "    \"\"\"\n",
    "    Run model on a single feature tensor.\n",
    "    feat_tensor: [1,1,F,T] on device\n",
    "    returns: (pred_class, confidence, full_probs)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(feat_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        conf, idx = torch.max(probs, dim=0)\n",
    "        pred_class = ALL_CLASSES[idx.item()]\n",
    "        return pred_class, conf.item(), probs.cpu().numpy()\n",
    "\n",
    "\n",
    "def demo_on_file(path, model, noise_waveforms=None, snr_db=None):\n",
    "    \"\"\"\n",
    "    Run GuardianDrive-Voice model on:\n",
    "      - clean version\n",
    "      - optional noisy version (if noise_waveforms and snr_db provided)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== GuardianDrive Demo on: {path} ===\")\n",
    "\n",
    "    # Clean\n",
    "    clean_wav, clean_feat = load_wav_and_features(path, mode=\"logmel\")\n",
    "    clean_X = clean_feat.unsqueeze(0).to(device)  # [1,1,F,T]\n",
    "    pred_clean, conf_clean, _ = guardian_predict(model, clean_X)\n",
    "    print(f\"Clean           -> predicted: {pred_clean:>7s}, confidence: {conf_clean*100:5.2f}%\")\n",
    "\n",
    "    # Noisy version (if noise + SNR specified)\n",
    "    if noise_waveforms and snr_db is not None:\n",
    "        noise = random.choice(noise_waveforms)\n",
    "        noisy_wav = mix_with_noise(clean_wav, noise, snr_db)\n",
    "        noisy_feat = waveform_to_features(noisy_wav, mode=\"logmel\")\n",
    "        noisy_X = noisy_feat.unsqueeze(0).to(device)\n",
    "        pred_noisy, conf_noisy, _ = guardian_predict(model, noisy_X)\n",
    "        print(f\"Noise @ {snr_db:>4.1f} dB -> predicted: {pred_noisy:>7s}, confidence: {conf_noisy*100:5.2f}%\")\n",
    "\n",
    "\n",
    "# Use BEST model: log-Mel + noise + SpecAugment\n",
    "guardian_model = model_spec\n",
    "\n",
    "# Adjust these paths to wherever your demo wavs are\n",
    "DEMO_FILES = [\n",
    "    # \"/path/to/guardian_help.wav\",\n",
    "    # \"/path/to/call_ambulance.wav\",\n",
    "    # \"/path/to/im_not_okay.wav\",\n",
    "]\n",
    "\n",
    "for fname in DEMO_FILES:\n",
    "    demo_on_file(fname, guardian_model, noise_waveforms=NOISE_WAVEFORMS, snr_db=10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 14.a RAVDESS DISTRESS DATASET (SPEAKER-INDEPENDENT)\n",
    "# ==============================\n",
    "\n",
    "class RAVDESSDistressDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Binary classification on RAVDESS:\n",
    "      y = 0 -> non-distress (neutral, calm, happy, sad, disgust, surprised)\n",
    "      y = 1 -> distress (angry, fearful)\n",
    "    Speaker-independent split:\n",
    "      Actors  1–18 -> train\n",
    "      Actors 19–21 -> val\n",
    "      Actors 22–24 -> test\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,              # path like \"./data/ravdess/audio_speech_actors_01-24\"\n",
    "        split: str = \"train\",   # \"train\" | \"val\" | \"test\"\n",
    "        feature_mode: str = \"logmel\",\n",
    "        sample_rate: int = SAMPLE_RATE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.feature_mode = feature_mode\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.train_actors = set(range(1, 19))\n",
    "        self.val_actors   = set(range(19, 22))\n",
    "        self.test_actors  = set(range(22, 25))\n",
    "\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "\n",
    "        all_wavs = sorted(self.root.rglob(\"*.wav\"))\n",
    "\n",
    "        if len(all_wavs) == 0:\n",
    "            print(f\"[WARN] No .wav files found under {root}. Check the path.\")\n",
    "        else:\n",
    "            print(f\"[RAVDESS] Found total wav files: {len(all_wavs)}\")\n",
    "\n",
    "        for wav_path in all_wavs:\n",
    "            actor_folder = wav_path.parent.name      # \"Actor_01\"\n",
    "            try:\n",
    "                actor_id = int(actor_folder.split(\"_\")[1])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if actor_id in self.train_actors:\n",
    "                actor_split = \"train\"\n",
    "            elif actor_id in self.val_actors:\n",
    "                actor_split = \"val\"\n",
    "            elif actor_id in self.test_actors:\n",
    "                actor_split = \"test\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if actor_split != split:\n",
    "                continue\n",
    "\n",
    "            label = self._emotion_to_distress(wav_path.name)\n",
    "            self.files.append(wav_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        print(f\"[RAVDESSDistressDataset] split={split}, size={len(self.files)}\")\n",
    "\n",
    "    def _emotion_to_distress(self, filename: str):\n",
    "        \"\"\"\n",
    "        RAVDESS file names like \"03-01-08-01-02-01-01.wav\"\n",
    "        3rd field = emotion_id.\n",
    "\n",
    "        Mapping:\n",
    "          distress (1)      -> {angry (05), fearful (06)}\n",
    "          non-distress (0)  -> everything else\n",
    "        \"\"\"\n",
    "        stem = filename.split(\".\")[0]\n",
    "        parts = stem.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            return 0\n",
    "        emotion_id = int(parts[2])\n",
    "\n",
    "        if emotion_id in {5, 6}:  # angry, fearful\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.files[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(str(wav_path))\n",
    "        waveform = pad_or_trim(waveform, waveform.shape[-1])\n",
    "\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        waveform = pad_or_trim(waveform, CLIP_NUM_SAMPLES)  # [1, 16000]\n",
    "\n",
    "        feat = waveform_to_features(waveform, mode=self.feature_mode)  # [1, F, T]\n",
    "\n",
    "        return feat, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 14b. DISTRESS DATALOADERS\n",
    "# ==============================\n",
    "\n",
    "# Quick check that RAVDESS root exists\n",
    "print(\"RAVDESS root exists:\", os.path.isdir(RAVDESS_ROOT))\n",
    "\n",
    "dist_train_ds = RAVDESSDistressDataset(RAVDESS_ROOT, split=\"train\", feature_mode=\"logmel\")\n",
    "dist_val_ds   = RAVDESSDistressDataset(RAVDESS_ROOT, split=\"val\",   feature_mode=\"logmel\")\n",
    "dist_test_ds  = RAVDESSDistressDataset(RAVDESS_ROOT, split=\"test\",  feature_mode=\"logmel\")\n",
    "\n",
    "dist_train_loader = DataLoader(\n",
    "    dist_train_ds, batch_size=64, shuffle=True,\n",
    "    collate_fn=guardian_collate_fn, num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "dist_val_loader = DataLoader(\n",
    "    dist_val_ds, batch_size=64, shuffle=False,\n",
    "    collate_fn=guardian_collate_fn, num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "dist_test_loader = DataLoader(\n",
    "    dist_test_ds, batch_size=64, shuffle=False,\n",
    "    collate_fn=guardian_collate_fn, num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Sanity check: one batch\n",
    "batch_X_dist, batch_y_dist = next(iter(dist_train_loader))\n",
    "print(\"Distress batch X shape:\", batch_X_dist.shape)  # [B,1,F,T]\n",
    "print(\"Distress batch y shape:\", batch_y_dist.shape)  # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 15. DISTRESS MODEL (SINGLE-TASK)\n",
    "# ==============================\n",
    "\n",
    "class DistressHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP head:\n",
    "      128-dim embedding -> 64-dim -> 2-class logits\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class DistressVoiceModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Distress classifier using the same SharedAudioEncoder as KWS.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SharedAudioEncoder(n_mels=N_MELS)\n",
    "        self.distress_head = DistressHead(self.encoder.embedding_dim, num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)              # [B, 128]\n",
    "        logits = self.distress_head(z)   # [B, 2]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 16. TRAIN DISTRESS MODEL (SINGLE-TASK) + CONFUSION MATRIX\n",
    "# ==============================\n",
    "\n",
    "dist_model = DistressVoiceModel().to(device)\n",
    "\n",
    "criterion_dist = nn.CrossEntropyLoss()\n",
    "optimizer_dist = torch.optim.Adam(dist_model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_dist = 5   # can increase to 10–15 later\n",
    "\n",
    "for epoch in range(1, num_epochs_dist + 1):\n",
    "    train_loss_d, train_acc_d = train_one_epoch(\n",
    "        dist_model, dist_train_loader, optimizer_dist, criterion_dist, device\n",
    "    )\n",
    "    val_loss_d, val_acc_d = evaluate(\n",
    "        dist_model, dist_val_loader, criterion_dist, device\n",
    "    )\n",
    "\n",
    "    print(f\"[DISTRESS] Epoch {epoch:02d}: \"\n",
    "          f\"Train loss {train_loss_d:.4f}, acc {train_acc_d*100:.2f}% | \"\n",
    "          f\"Val loss {val_loss_d:.4f}, acc {val_acc_d*100:.2f}%\")\n",
    "\n",
    "test_loss_d, test_acc_d = evaluate(\n",
    "    dist_model, dist_test_loader, criterion_dist, device\n",
    ")\n",
    "print(f\"[DISTRESS] Final Test loss {test_loss_d:.4f}, acc {test_acc_d*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix / F1\n",
    "dist_model.eval()\n",
    "\n",
    "all_y = []\n",
    "all_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in dist_test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = dist_model(X)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_pred.append(preds.cpu().numpy())\n",
    "\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "\n",
    "print(\"Confusion matrix (distress model):\")\n",
    "print(confusion_matrix(all_y, all_pred))\n",
    "\n",
    "print(\"\\nClassification report (distress model):\")\n",
    "print(classification_report(all_y, all_pred,\n",
    "                            target_names=[\"non_distress\", \"distress\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 17. MULTI-TASK MODEL (KWS + DISTRESS)\n",
    "# ==============================\n",
    "\n",
    "class MultiTaskGuardianModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared encoder with two heads:\n",
    "      - command_head  : 11-way KWS (yes, no, up, ..., unknown)\n",
    "      - distress_head : 2-way (non_distress, distress)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_command_classes: int, num_distress_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.encoder = SharedAudioEncoder(n_mels=N_MELS)\n",
    "        self.command_head = CommandHead(self.encoder.embedding_dim,\n",
    "                                        num_command_classes)\n",
    "        self.distress_head = DistressHead(self.encoder.embedding_dim,\n",
    "                                          num_distress_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)                   # [B, 128]\n",
    "        cmd_logits = self.command_head(z)     # [B, 11]\n",
    "        dist_logits = self.distress_head(z)   # [B, 2]\n",
    "        return cmd_logits, dist_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 17a. MULTI-TASK TRAIN / EVAL HELPERS\n",
    "# ==============================\n",
    "\n",
    "def train_multitask_epoch(\n",
    "    model,\n",
    "    kws_loader,            # DataLoader for KWS (SpeechCommands)\n",
    "    dist_loader,           # DataLoader for Distress (RAVDESS)\n",
    "    optimizer,\n",
    "    cmd_criterion,\n",
    "    dist_criterion,\n",
    "    device,\n",
    "    lambda_dist: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    One epoch of multi-task training.\n",
    "    Each step sees one KWS batch and one Distress batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_cmd_loss = 0.0\n",
    "    total_dist_loss = 0.0\n",
    "    total_cmd_correct = 0\n",
    "    total_dist_correct = 0\n",
    "    total_cmd_samples = 0\n",
    "    total_dist_samples = 0\n",
    "\n",
    "    for (X_cmd, y_cmd), (X_dist, y_dist) in zip(kws_loader, dist_loader):\n",
    "        X_cmd = X_cmd.to(device)\n",
    "        y_cmd = y_cmd.to(device)\n",
    "        X_dist = X_dist.to(device)\n",
    "        y_dist = y_dist.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # KWS batch\n",
    "        cmd_logits, _ = model(X_cmd)\n",
    "        loss_cmd = cmd_criterion(cmd_logits, y_cmd)\n",
    "        cmd_preds = cmd_logits.argmax(dim=1)\n",
    "        cmd_correct = (cmd_preds == y_cmd).sum().item()\n",
    "\n",
    "        # Distress batch\n",
    "        _, dist_logits = model(X_dist)\n",
    "        loss_dist = dist_criterion(dist_logits, y_dist)\n",
    "        dist_preds = dist_logits.argmax(dim=1)\n",
    "        dist_correct = (dist_preds == y_dist).sum().item()\n",
    "\n",
    "        # Combined loss\n",
    "        loss = loss_cmd + lambda_dist * loss_dist\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_cmd_size = y_cmd.size(0)\n",
    "        batch_dist_size = y_dist.size(0)\n",
    "\n",
    "        total_cmd_loss += loss_cmd.item() * batch_cmd_size\n",
    "        total_dist_loss += loss_dist.item() * batch_dist_size\n",
    "\n",
    "        total_cmd_correct += cmd_correct\n",
    "        total_dist_correct += dist_correct\n",
    "\n",
    "        total_cmd_samples += batch_cmd_size\n",
    "        total_dist_samples += batch_dist_size\n",
    "\n",
    "    avg_cmd_loss = total_cmd_loss / max(1, total_cmd_samples)\n",
    "    avg_dist_loss = total_dist_loss / max(1, total_dist_samples)\n",
    "\n",
    "    cmd_acc = total_cmd_correct / max(1, total_cmd_samples)\n",
    "    dist_acc = total_dist_correct / max(1, total_dist_samples)\n",
    "\n",
    "    return avg_cmd_loss, cmd_acc, avg_dist_loss, dist_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_kws_multitask(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        cmd_logits, _ = model(X)\n",
    "        preds = cmd_logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return correct / max(1, total)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_distress_multitask(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        _, dist_logits = model(X)\n",
    "        preds = dist_logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return correct / max(1, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 17b. RUN MULTI-TASK TRAINING\n",
    "# ==============================\n",
    "\n",
    "# Reuse KWS loaders from EXP 2 (noise-trained log-Mel)\n",
    "kws_train_loader_mt = train_loader_noise\n",
    "kws_val_loader_mt   = val_loader_logmel_noise\n",
    "kws_test_loader_mt  = test_loader_logmel_noise\n",
    "\n",
    "multitask_model = MultiTaskGuardianModel(\n",
    "    num_command_classes=NUM_CLASSES,\n",
    "    num_distress_classes=2,\n",
    ").to(device)\n",
    "\n",
    "cmd_criterion = nn.CrossEntropyLoss()\n",
    "dist_criterion = nn.CrossEntropyLoss()\n",
    "optimizer_mt = torch.optim.Adam(multitask_model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_mt = 5\n",
    "lambda_dist = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs_mt + 1):\n",
    "    cmd_loss, cmd_acc, dist_loss, dist_acc = train_multitask_epoch(\n",
    "        multitask_model,\n",
    "        kws_train_loader_mt,\n",
    "        dist_train_loader,\n",
    "        optimizer_mt,\n",
    "        cmd_criterion,\n",
    "        dist_criterion,\n",
    "        device,\n",
    "        lambda_dist=lambda_dist,\n",
    "    )\n",
    "\n",
    "    cmd_val_acc = evaluate_kws_multitask(multitask_model, kws_val_loader_mt, device)\n",
    "    dist_val_acc = evaluate_distress_multitask(multitask_model, dist_val_loader, device)\n",
    "\n",
    "    print(f\"[MULTI-TASK] Epoch {epoch:02d}: \"\n",
    "          f\"KWS train loss {cmd_loss:.4f}, train acc {cmd_acc*100:.2f}% | \"\n",
    "          f\"Distress train loss {dist_loss:.4f}, train acc {dist_acc*100:.2f}% || \"\n",
    "          f\"KWS val acc {cmd_val_acc*100:.2f}% | Distress val acc {dist_val_acc*100:.2f}%\")\n",
    "\n",
    "kws_test_acc_mt = evaluate_kws_multitask(multitask_model, kws_test_loader_mt, device)\n",
    "dist_test_acc_mt = evaluate_distress_multitask(multitask_model, dist_test_loader, device)\n",
    "\n",
    "print(f\"[MULTI-TASK] Final KWS test acc: {kws_test_acc_mt*100:.2f}%\")\n",
    "print(f\"[MULTI-TASK] Final Distress test acc: {dist_test_acc_mt*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 18. REAL-TIME AUDIO LAYER + DEPLOYMENT + VISUALIZATION (UPDATED)\n",
    "#     - Robust noise loading (recursive) \n",
    "#     - Front-end conditioning (bandpass + AGC)\n",
    "#     - VAD + endpointing (energy-based)\n",
    "#     - Streaming inference simulation + wake-gate\n",
    "#     - Export (TorchScript + ONNX) + quantization demo\n",
    "#     - Latency profiling (CUDA + MPS sync) + model size\n",
    "#     - Better visualizations (raw + normalized confusion matrices, VAD energy plot)\n",
    "# ==============================\n",
    "\n",
    "import time\n",
    "\n",
    "# ---------- 18.1 Robust noise loader (override) + reload ----------\n",
    "def load_noise_waveforms(noise_dir: str, sample_rate: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Robust noise loader:\n",
    "      - recursive search\n",
    "      - case-insensitive .wav\n",
    "      - converts to mono, resamples, DC-removes\n",
    "    Returns: list of waveforms, each [1, T_noise]\n",
    "    \"\"\"\n",
    "    noise_waveforms = []\n",
    "    noise_dir_path = Path(noise_dir)\n",
    "\n",
    "    if not noise_dir_path.exists():\n",
    "        print(f\"[18][WARN] Noise directory does not exist: {noise_dir}\")\n",
    "        return noise_waveforms\n",
    "\n",
    "    candidates = [p for p in noise_dir_path.rglob(\"*\")\n",
    "                  if p.is_file() and p.suffix.lower() == \".wav\"]\n",
    "\n",
    "    for wav_path in candidates:\n",
    "        try:\n",
    "            wav, sr = torchaudio.load(str(wav_path))\n",
    "\n",
    "            # force mono [1, T]\n",
    "            if wav.ndim == 2 and wav.shape[0] > 1:\n",
    "                wav = wav.mean(dim=0, keepdim=True)\n",
    "            elif wav.ndim == 1:\n",
    "                wav = wav.unsqueeze(0)\n",
    "\n",
    "            if sr != sample_rate:\n",
    "                wav = torchaudio.functional.resample(wav, sr, sample_rate)\n",
    "\n",
    "            wav = wav - wav.mean()\n",
    "            noise_waveforms.append(wav)\n",
    "        except Exception as e:\n",
    "            print(f\"[18][WARN] Failed noise file {wav_path}: {e}\")\n",
    "\n",
    "    print(f\"[18] Loaded {len(noise_waveforms)} noise files from {noise_dir} (recursive)\")\n",
    "    return noise_waveforms\n",
    "\n",
    "# Reload noise waveforms for streaming / noisy test loaders created AFTER this point\n",
    "NOISE_WAVEFORMS = load_noise_waveforms(NOISE_DIR, SAMPLE_RATE)\n",
    "if len(NOISE_WAVEFORMS) == 0:\n",
    "    print(\"[18][WARN] NOISE_WAVEFORMS is empty. Noise mixing will be disabled in demos/loaders built after this.\")\n",
    "\n",
    "\n",
    "# ---------- 18.2 Front-end conditioning: bandpass + AGC ----------\n",
    "def _to_mono_1ch(wav: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure mono [1, T].\"\"\"\n",
    "    if wav.ndim == 2 and wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    elif wav.ndim == 1:\n",
    "        wav = wav.unsqueeze(0)\n",
    "    return wav\n",
    "\n",
    "def agc_rms_normalize(wav: torch.Tensor, target_dbfs: float = -20.0, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Simple AGC: normalize RMS to target dBFS (wav assumed in [-1,1]).\n",
    "    \"\"\"\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    rms = torch.sqrt(torch.mean(wav ** 2) + eps)\n",
    "    target_rms = 10 ** (target_dbfs / 20.0)\n",
    "    gain = target_rms / rms\n",
    "    out = torch.clamp(wav * gain, -1.0, 1.0)\n",
    "    return out\n",
    "\n",
    "def simple_bandpass_condition(wav: torch.Tensor, sr: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Cheap speech-band conditioning (NOT a real denoiser):\n",
    "      - highpass ~80 Hz\n",
    "      - lowpass  ~7.6 kHz\n",
    "    \"\"\"\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    wav = torchaudio.functional.highpass_biquad(wav, sr, cutoff_freq=80.0)\n",
    "    wav = torchaudio.functional.lowpass_biquad(wav, sr, cutoff_freq=7600.0)\n",
    "    return wav\n",
    "\n",
    "def frontend_process(wav: torch.Tensor, sr: int = SAMPLE_RATE,\n",
    "                     do_bandpass: bool = True, do_agc: bool = True):\n",
    "    \"\"\"\n",
    "    Front-end conditioning chain.\n",
    "    \"\"\"\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    if do_bandpass:\n",
    "        wav = simple_bandpass_condition(wav, sr)\n",
    "    if do_agc:\n",
    "        wav = agc_rms_normalize(wav, target_dbfs=-20.0)\n",
    "    return wav\n",
    "\n",
    "\n",
    "# ---------- 18.3 VAD + endpointing (energy-based) ----------\n",
    "def vad_energy_segments(\n",
    "    wav: torch.Tensor,\n",
    "    sr: int = SAMPLE_RATE,\n",
    "    frame_ms: float = 30.0,\n",
    "    hop_ms: float = 10.0,\n",
    "    energy_threshold_ratio: float = 2.5,\n",
    "    min_speech_ms: float = 200.0,\n",
    "    hangover_ms: float = 300.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns list of (start_sample, end_sample) speech segments.\n",
    "    Energy threshold is median_energy * ratio (robust-ish).\n",
    "    \"\"\"\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    x = wav[0]\n",
    "\n",
    "    frame = int(sr * frame_ms / 1000.0)\n",
    "    hop = int(sr * hop_ms / 1000.0)\n",
    "    if frame <= 0 or hop <= 0 or x.numel() < frame:\n",
    "        return [], None\n",
    "\n",
    "    energies, idxs = [], []\n",
    "    for i in range(0, x.numel() - frame + 1, hop):\n",
    "        chunk = x[i:i+frame]\n",
    "        energies.append((chunk ** 2).mean().item())\n",
    "        idxs.append(i)\n",
    "\n",
    "    med = float(np.median(energies)) + 1e-12\n",
    "    thr = med * energy_threshold_ratio\n",
    "\n",
    "    min_speech_frames = max(1, int(min_speech_ms / hop_ms))\n",
    "    hangover_frames = max(1, int(hangover_ms / hop_ms))\n",
    "\n",
    "    segments = []\n",
    "    in_speech = False\n",
    "    speech_start = None\n",
    "    speech_frames = 0\n",
    "    silence_frames = 0\n",
    "\n",
    "    for k, e in enumerate(energies):\n",
    "        is_speech = e > thr\n",
    "\n",
    "        if not in_speech:\n",
    "            if is_speech:\n",
    "                speech_frames += 1\n",
    "                if speech_frames >= min_speech_frames:\n",
    "                    in_speech = True\n",
    "                    speech_start = idxs[max(0, k - min_speech_frames + 1)]\n",
    "                    silence_frames = 0\n",
    "            else:\n",
    "                speech_frames = 0\n",
    "        else:\n",
    "            if is_speech:\n",
    "                silence_frames = 0\n",
    "            else:\n",
    "                silence_frames += 1\n",
    "                if silence_frames >= hangover_frames:\n",
    "                    speech_end = idxs[k] + frame\n",
    "                    segments.append((speech_start, min(speech_end, x.numel())))\n",
    "                    in_speech = False\n",
    "                    speech_start = None\n",
    "                    speech_frames = 0\n",
    "                    silence_frames = 0\n",
    "\n",
    "    if in_speech and speech_start is not None:\n",
    "        segments.append((speech_start, x.numel()))\n",
    "\n",
    "    meta = {\n",
    "        \"energies\": np.array(energies, dtype=np.float32),\n",
    "        \"idxs\": np.array(idxs, dtype=np.int64),\n",
    "        \"thr\": float(thr),\n",
    "        \"frame\": frame,\n",
    "        \"hop\": hop,\n",
    "        \"frame_ms\": frame_ms,\n",
    "        \"hop_ms\": hop_ms,\n",
    "    }\n",
    "    return segments, meta\n",
    "\n",
    "\n",
    "# ---------- 18.4 Visualization utilities ----------\n",
    "def plot_confusion_matrix(cm: np.ndarray, class_names, title: str, normalize: bool = False):\n",
    "    \"\"\"\n",
    "    If normalize=True: row-normalized (percent per true class).\n",
    "    \"\"\"\n",
    "    cm_plot = cm.astype(np.float32)\n",
    "\n",
    "    if normalize:\n",
    "        row_sums = cm_plot.sum(axis=1, keepdims=True) + 1e-12\n",
    "        cm_plot = (cm_plot / row_sums) * 100.0  # percent\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.imshow(cm_plot, interpolation=\"nearest\")\n",
    "    plt.title(title + (\" (Normalized %)\" if normalize else \"\"))\n",
    "    plt.colorbar()\n",
    "\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_vad_segments(wav: torch.Tensor, sr: int, segments, title=\"VAD Segments\"):\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    x = wav[0].cpu().numpy()\n",
    "    t = np.arange(len(x)) / sr\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(t, x)\n",
    "    for (s0, s1) in segments:\n",
    "        plt.axvspan(s0/sr, s1/sr, alpha=0.25)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_vad_energy(meta, title=\"VAD Frame Energy\"):\n",
    "    if meta is None:\n",
    "        print(\" No VAD meta to plot.\")\n",
    "        return\n",
    "    energies = meta[\"energies\"]\n",
    "    idxs = meta[\"idxs\"]\n",
    "    thr = meta[\"thr\"]\n",
    "\n",
    "    times = idxs / SAMPLE_RATE\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(times, energies)\n",
    "    plt.axhline(thr, linestyle=\"--\", linewidth=1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frame Energy\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_logmel(feat: torch.Tensor, title=\"Log-Mel\"):\n",
    "    # feat: [1, F, T]\n",
    "    m = feat[0].detach().cpu().numpy()\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(m, aspect=\"auto\", origin=\"lower\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Mel bins\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------- 18.5 Streaming inference simulation + wake-gate ----------\n",
    "def streaming_kws_demo(\n",
    "    wav_path: str,\n",
    "    model: nn.Module,\n",
    "    wake_gate_class: str = \"go\",\n",
    "    wake_prob_thresh: float = 0.85,\n",
    "    step_ms: float = 100.0,\n",
    "    window_sec: float = 1.0,\n",
    "    do_frontend: bool = True,\n",
    "    do_viz: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Offline streaming demo:\n",
    "      - front-end conditioning\n",
    "      - VAD segmentation\n",
    "      - slide 1s windows through VAD speech segments\n",
    "      - trigger if predicted class == wake_gate_class with confidence >= threshold\n",
    "\n",
    "    NOTE: This is NOT a trained wake-word detector. It's a demo-friendly gate.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(wav_path):\n",
    "        print(f\" File not found: {wav_path}\")\n",
    "        return []\n",
    "\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = _to_mono_1ch(wav)\n",
    "\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "        sr = SAMPLE_RATE\n",
    "\n",
    "    if do_frontend:\n",
    "        wav = frontend_process(wav, sr)\n",
    "\n",
    "    segments, meta = vad_energy_segments(wav, sr)\n",
    "    print(f\" VAD segments found: {len(segments)}\")\n",
    "\n",
    "    if do_viz:\n",
    "        plot_vad_segments(wav, sr, segments, title=f\"VAD Segments: {os.path.basename(wav_path)}\")\n",
    "        plot_vad_energy(meta, title=f\"VAD Energy: {os.path.basename(wav_path)}\")\n",
    "\n",
    "    wake_idx = CLASS_TO_INDEX.get(wake_gate_class, None)\n",
    "    if wake_idx is None:\n",
    "        print(f\" wake_gate_class '{wake_gate_class}' not in ALL_CLASSES.\")\n",
    "        return []\n",
    "\n",
    "    step = int(sr * step_ms / 1000.0)\n",
    "    win = int(sr * window_sec)\n",
    "\n",
    "    model.eval()\n",
    "    triggers = []  # dicts: {\"time\": float, \"pred\": str, \"conf\": float}\n",
    "\n",
    "    for (s0, s1) in segments:\n",
    "        pos = s0\n",
    "        while pos + win <= s1:\n",
    "            chunk = wav[:, pos:pos+win]\n",
    "            feat = waveform_to_features(chunk, mode=\"logmel\")  # [1,F,T]\n",
    "            X = feat.unsqueeze(0).to(device)                   # [1,1,F,T]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(X)\n",
    "                probs = torch.softmax(logits, dim=1)[0]\n",
    "                conf, pred = torch.max(probs, dim=0)\n",
    "\n",
    "            pred_name = ALL_CLASSES[pred.item()]\n",
    "            if pred.item() == wake_idx and conf.item() >= wake_prob_thresh:\n",
    "                tsec = pos / sr\n",
    "                triggers.append({\"time\": tsec, \"pred\": pred_name, \"conf\": conf.item()})\n",
    "                print(f\"[WAKE-GATE] {tsec:6.2f}s -> {pred_name} ({conf.item()*100:5.1f}%)\")\n",
    "\n",
    "            pos += step\n",
    "\n",
    "    if do_viz:\n",
    "        if len(triggers) > 0:\n",
    "            times = [d[\"time\"] for d in triggers]\n",
    "            confs = [d[\"conf\"] * 100 for d in triggers]\n",
    "            plt.figure(figsize=(10, 2.5))\n",
    "            plt.scatter(times, confs)\n",
    "            plt.title(\"Wake-Gate Triggers Over Time\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Confidence (%)\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\" No wake-gate triggers at current threshold.\")\n",
    "\n",
    "    return triggers\n",
    "\n",
    "\n",
    "# ---------- 18.6 Deployment: quantization + export ----------\n",
    "def quantize_for_cpu(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic quantization demo: affects Linear layers only (limited gains here).\n",
    "    \"\"\"\n",
    "    model_cpu = model.to(\"cpu\").eval()\n",
    "    qmodel = torch.quantization.quantize_dynamic(\n",
    "        model_cpu,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    return qmodel\n",
    "\n",
    "def export_torchscript(model: nn.Module, out_path=\"guardian_voice_kws.pt\"):\n",
    "    model_cpu = model.to(\"cpu\").eval()\n",
    "    example = torch.randn(1, 1, N_MELS, 101)\n",
    "    traced = torch.jit.trace(model_cpu, example)\n",
    "    traced.save(out_path)\n",
    "    print(f\" Saved TorchScript: {out_path}\")\n",
    "\n",
    "def export_onnx(model: nn.Module, out_path=\"guardian_voice_kws.onnx\"):\n",
    "    model_cpu = model.to(\"cpu\").eval()\n",
    "    example = torch.randn(1, 1, N_MELS, 101)\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model_cpu,\n",
    "            example,\n",
    "            out_path,\n",
    "            input_names=[\"x\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\"x\": {0: \"batch\", 3: \"time\"}, \"logits\": {0: \"batch\"}},\n",
    "            opset_version=17\n",
    "        )\n",
    "        print(f\" Saved ONNX: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] ONNX export failed: {e}\")\n",
    "\n",
    "\n",
    "# ---------- 18.7 Profiling: latency + model size ----------\n",
    "def _count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def _model_size_mb_state_dict(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Rough size: sum of state_dict tensor bytes / MB.\n",
    "    \"\"\"\n",
    "    total_bytes = 0\n",
    "    sd = model.state_dict()\n",
    "    for k, v in sd.items():\n",
    "        if torch.is_tensor(v):\n",
    "            total_bytes += v.numel() * v.element_size()\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "def benchmark_forward(model: nn.Module, device: torch.device, reps: int = 300, warmup: int = 50):\n",
    "    \"\"\"\n",
    "    Forward-only benchmark. Includes correct synchronization for CUDA and MPS.\n",
    "    \"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    x = torch.randn(1, 1, N_MELS, 101, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = model(x)\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "        ms = (t1 - t0) * 1000 / reps\n",
    "        mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        print(f\"[CUDA] forward avg: {ms:.3f} ms | peak mem: {mem:.1f} MB\")\n",
    "\n",
    "    elif device.type == \"mps\":\n",
    "        # MPS is async: must sync\n",
    "        torch.mps.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = model(x)\n",
    "        torch.mps.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "        ms = (t1 - t0) * 1000 / reps\n",
    "        print(f\"[MPS] forward avg: {ms:.3f} ms\")\n",
    "\n",
    "    else:\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = model(x)\n",
    "        t1 = time.perf_counter()\n",
    "        ms = (t1 - t0) * 1000 / reps\n",
    "        print(f\"[CPU] forward avg: {ms:.3f} ms\")\n",
    "\n",
    "    print(f\" Params: {_count_params(model):,} | StateDict size: {_model_size_mb_state_dict(model):.2f} MB\")\n",
    "\n",
    "def benchmark_end_to_end_kws(model: nn.Module, device: torch.device, reps: int = 50):\n",
    "    \"\"\"\n",
    "    End-to-end: waveform -> features -> forward.\n",
    "    This is closer to real streaming cost than forward-only.\n",
    "    \"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    wav = torch.randn(1, CLIP_NUM_SAMPLES) * 0.05  # 1 sec synthetic \"audio\"\n",
    "    wav = frontend_process(wav, SAMPLE_RATE)       # front-end on CPU\n",
    "    wav = wav.to(\"cpu\")\n",
    "\n",
    "    # time features on CPU + forward on device\n",
    "    times = []\n",
    "    for _ in range(reps):\n",
    "        t0 = time.perf_counter()\n",
    "        feat = waveform_to_features(wav, mode=\"logmel\")        # [1,F,T] CPU\n",
    "        X = feat.unsqueeze(0).to(device)                      # [1,1,F,T]\n",
    "        _ = model(X)                                          # forward\n",
    "        # sync if needed\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        elif device.type == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append((t1 - t0) * 1000)\n",
    "\n",
    "    print(f\" End-to-end (feat+forward) avg: {np.mean(times):.2f} ms | p95: {np.percentile(times, 95):.2f} ms\")\n",
    "\n",
    "\n",
    "# ---------- 18.8 Safety checks (non-production heuristics) ----------\n",
    "def simple_replay_risk_checks(wav: torch.Tensor, sr: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    NON-PRODUCTION heuristics:\n",
    "      - clipping rate\n",
    "      - energy coefficient-of-variation (low variability can indicate replay/processed audio)\n",
    "    \"\"\"\n",
    "    wav = _to_mono_1ch(wav)\n",
    "    x = wav[0]\n",
    "\n",
    "    clip_rate = (x.abs() > 0.98).float().mean().item()\n",
    "\n",
    "    frame = int(sr * 0.03)\n",
    "    hop = int(sr * 0.01)\n",
    "    energies = []\n",
    "    for i in range(0, max(1, x.numel() - frame), hop):\n",
    "        chunk = x[i:i+frame]\n",
    "        energies.append((chunk**2).mean().item())\n",
    "    energies = np.array(energies) if len(energies) else np.array([0.0])\n",
    "\n",
    "    e_std = float(np.std(energies))\n",
    "    e_mean = float(np.mean(energies) + 1e-12)\n",
    "    cv = e_std / e_mean\n",
    "\n",
    "    warnings = []\n",
    "    if clip_rate > 0.01:\n",
    "        warnings.append(f\"High clipping rate: {clip_rate*100:.2f}%\")\n",
    "    if cv < 0.15:\n",
    "        warnings.append(f\"Low energy variability (CV={cv:.2f}) — possible replay/processed audio\")\n",
    "    return warnings\n",
    "\n",
    "\n",
    "# ---------- 18.9 Better confusion-matrix visualizations (raw + normalized) ----------\n",
    "# KWS CM plots (SpecAug best model)\n",
    "try:\n",
    "    model_spec.eval()\n",
    "    all_y_kws, all_pred_kws = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader_logmel_spec:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model_spec(X)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_y_kws.append(y.cpu().numpy())\n",
    "            all_pred_kws.append(preds.cpu().numpy())\n",
    "\n",
    "    all_y_kws = np.concatenate(all_y_kws)\n",
    "    all_pred_kws = np.concatenate(all_pred_kws)\n",
    "    cm_kws = confusion_matrix(all_y_kws, all_pred_kws)\n",
    "\n",
    "    plot_confusion_matrix(cm_kws, ALL_CLASSES, \"KWS Confusion Matrix (SpecAug Log-Mel)\", normalize=False)\n",
    "    plot_confusion_matrix(cm_kws, ALL_CLASSES, \"KWS Confusion Matrix (SpecAug Log-Mel)\", normalize=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\" Skipping KWS CM plots (model_spec/test_loader not available):\", e)\n",
    "\n",
    "# Distress CM plots (dist_model)\n",
    "try:\n",
    "    dist_model.eval()\n",
    "    all_y_d, all_pred_d = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dist_test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = dist_model(X)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_y_d.append(y.cpu().numpy())\n",
    "            all_pred_d.append(preds.cpu().numpy())\n",
    "\n",
    "    all_y_d = np.concatenate(all_y_d)\n",
    "    all_pred_d = np.concatenate(all_pred_d)\n",
    "    cm_d = confusion_matrix(all_y_d, all_pred_d)\n",
    "\n",
    "    plot_confusion_matrix(cm_d, [\"non_distress\", \"distress\"], \"Distress Confusion Matrix\", normalize=False)\n",
    "    plot_confusion_matrix(cm_d, [\"non_distress\", \"distress\"], \"Distress Confusion Matrix\", normalize=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\" Skipping Distress CM plots (dist_model/dist_test_loader not available):\", e)\n",
    "\n",
    "\n",
    "# ---------- 18.10 Quick run: benchmark + optional exports ----------\n",
    "try:\n",
    "    print(\"\\n Forward benchmark (guardian_model if defined, else model_spec)...\")\n",
    "    _m = guardian_model if \"guardian_model\" in globals() else model_spec\n",
    "    benchmark_forward(_m, device)\n",
    "    benchmark_end_to_end_kws(_m, device)\n",
    "\n",
    "    # Optional exports\n",
    "    # export_torchscript(_m, \"guardian_voice_kws.pt\")\n",
    "    # export_onnx(_m, \"guardian_voice_kws.onnx\")\n",
    "\n",
    "    # Optional quantization demo (CPU only)\n",
    "    # q_m = quantize_for_cpu(_m)\n",
    "    # print(\"[18] Quantized model (CPU) ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\" Benchmark/export section skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ASR Project (venv)",
   "language": "python",
   "name": "asr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
